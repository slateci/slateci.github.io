---
title: "Deploying and testing an OSG Hosted CE via SLATE"
overview: Blog
published: true
permalink: blog/deploy-hosted-ce.html
attribution: The SLATE Team
layout: post
type: markdown
tag: draft
---

# Deploying an OSG HostedCE

## Prerequisites
The OSG HostedCE uses SSH to submit pilot jobs that will connect back to the
OSG central pool. In order for the HostedCE to work, you'll first need to
create a local service account for OSG to submit jobs. This should be done
according to whatever process you normally use to create accounts for users.

## Generating and storing the key
Once the account has been created, you'll want to create a new SSH key pair.
The private part of the key will be stored within SLATE, and the public part of
the key will be installed into `authorized_keys` file of the OSG user on your
cluster. To generate the key, you'll need to run the following on some machine
with OpenSSH installed:

	ssh-keygen -f osg-keypair

Note that you will need to make this key passphraseless, as the HostedCE
software will consume this key. Once you've created the key, you'll want to
store the public part of it (osg-keypair.pub) into the `authorized_keys` file
on the OSG account for your cluster. For example, if your OSG service account
is called `osg`, you'll want to append the contents of `osg-keypair.pub` to
`/home/osg/.ssh/authorized_keys`. 

The private part of the keypair will need to be stored on a SLATE cluster for use by the
CE. In this particular example, I'll be operating under the `slate-dev` group and using the `uutah-prod` cluster to host a CE pointed at a SLURM cluster at University of Utah. To do that:

	slate secret create utah-lp-hostedce-privkey --from-file=bosco.key=osg-keypair --group slate-dev --cluster uutah-prod

Where `utah-lp-hostedce-privkey` will be the name of the secret, and `osg-keypair` is the
path to your private key (assumed to be the current working directory).

## Configuring the CE from SLATE
You'll want to download the application configuration template:

	slate app get-conf --dev osg-hosted-ce > hosted-ce.yaml

There are several things that you'll need to edit here. 

### Site Section
First, for the site section you'll want to put in appropriate values that will
ultimately map onto your OSG Topology entry. Let's take a look at this group
first:
	
	Site:
  	  Resource: SLATE_US_UUTAH_LONEPEAK
 	  ResourceGroup: CHPC Group
  	  Sponsor: osg:100
  	  Contact: Mitchell Steinman
  	  ContactEmail: chpc-osg-support@lists.utah.edu
  	  City: Salt Lake City
  	  Country: United States
  	  Latitude: 40.7608
  	  Longitude: -111.891
	
Group and Sponsor are safe to leave as defaults if you plan to support OSG
users with your CE. From the [OSG Resource Registration
Documentation](https://opensciencegrid.org/docs/common/registration/), here's
the definition of Resource and ResourceGroup:

| Level          | Definition |
| -------------- | ----------- |
| Resource Group | A logical grouping of resources at a site. Production and testing resources must be placed into separate Resource Groups. | 
| Resource       | A host belonging to a resource group that provides grid services, e.g. Compute Elements, storage endpoints, or perfSonar hosts. A resource may provide more than one service.|

These will ultimately get mapped into the [OSG
Topology](https://topology.opensciencegrid.org/). Following those, you'll need
to update the Contact and location information as appropriate. The contact
information will be used to reach you in case there are any problems with your
CE or site.

### Cluster Section
Next we'll go through the Cluster section, this section defines some of the hardware specifications on the remote side. Memory should be the total per node memory available on the remote cluster. By default this is interpreted as megabytes but you can format this as `24G` if you wish. You can tailor the amount of memory to the lowest common denominator if you have a remote cluster with different kinds of nodes. The same principle applies for the CoresPerNode field. MaxWallTime is the maxmimum allowed CPU walltime for the job expressed in minutes by default (in this case it is 72 hours).

You can read more on the [OSG Docs](https://opensciencegrid.org/docs/other/configuration-with-osg-configure/#subcluster-resource-entry)

VOs are virtual orginizations within the OSG and they correspond to different research groups. AllowedVOs let's us specify which groups will run jobs through our CE.

Finally you'll need to refer to the private key we stored in SLATE previously.
I had called mine `utah-lp-hostedce-privkey`, so my configuration ends up
looking like this:

	Cluster:
	  PrivateKeySecret: utah-lp-hostedce-privkey # maps to SLATE secret
	  Memory: 24576
	  CoresPerNode: 4
	  MaxWallTime: 4320
	  AllowedVOs: osg, cms, atlas, glow, hcc, fermilab, ligo, virgo, sdcc, sphenix, gluex, icecube, xenon
 
	  

### Storage Section
For the Storage section, you'll want to define the location where the OSG Worker Node Client is installed and the location of temp/scratch space for your workers. The OSG Worker Node Client is expected to be installed in the osguser's home directory in a subdirectory called `bosco-osg-wn-client`. You need to expand the absolute path of the user's home directory. In my case, this is under a shared file system. The HostedCE SLATE application will install this Worker Node Client directory for you. For temp, I have a specific scratch directory I want the OSG to use. `/tmp` is also a typical location to use.

	Storage:
  	  GridDir: /uufs/chpc.utah.edu/common/home/osguserl/bosco-osg-wn-client
  	  WorkerNodeTemp: /scratch/local/.osgscratch



### Squid Section
If you have a Squid service running, you can ensure that your workers will
communicate with your local squid here. I will point my cluster to the local
squid I have deployed with SLATE:

	Squid:
	  Location: sl-uu-es1.slateci.io:31726

(It's also possible to launch a Squid service through SLATE, see
[here](https://portal.slateci.io/applications/osg-frontier-squid))

### Netowrking Sections
The HostedCE application requires both forward and reverse DNS resolution to be enabled for its publicly routable IP. Most SLATE clusters come pre-configured with a handful of "LoadBalancer" IP addresses that can be allocated automatically to different applications. You must set-up the DNS records for this address so it is a good idea to request a specific address from the pool. For Utah my network config looks like this:


	Networking:
  	  Hostname: "sl-uu-hce2.slateci.io"
 	  RequestIP: 155.101.6.238


### HTCondorCeConfig Section
TODO

### BoscoOverrides Section
TODO

### HTTPLogger Section
TODO

### VomsmapOverride Section
TODO

### GridmapOverride Section
TODO

### Certficate Section
TODO

### Developer Section
TODO

## Finalizing the configuration

Now that we've gone through the sections line-by-line, let's look at our completed configuration:

```
Instance: "lonepeak"

Site:
  Resource: SLATE_US_UUTAH_LONEPEAK
  ResourceGroup: CHPC Group
  Sponsor: osg:100
  Contact: Mitchell Steinman
  ContactEmail: chpc-osg-support@lists.utah.edu
  City: Salt Lake City
  Country: United States
  Latitude: 40.7608
  Longitude: -111.891

Cluster:
  PrivateKeySecret: utah-lp-hostedce-privkey # maps to SLATE secret
  Memory: 24000
  CoresPerNode: 4
  MaxWallTime: 4320
  AllowedVOs: osg, cms, atlas, glow, hcc, fermilab, ligo, virgo, sdcc, sphenix, gluex, icecube, xenon

Storage:
  GridDir: /uufs/chpc.utah.edu/common/home/osguserl/bosco-osg-wn-client
  WorkerNodeTemp: /scratch/local/.osgscratch

Squid:
  Location: sl-uu-es1.slateci.io:31726

Networking:
  Hostname: "sl-uu-hce2.slateci.io"
  RequestIP: 155.101.6.238

HTCondorCeConfig: |+
  JOB_ROUTER_ENTRIES @=jre
  [
    GridResource = "batch slurm osguserl@lonepeak1.chpc.utah.edu";
    Requirements = (Owner == "osguserl");
  ]
  @jre

BoscoOverrides:
  Enabled: true
  GitEndpoint: "https://github.com/slateci/utah-bosco.git"
  RepoNeedsPrivKey: false
  GitKeySecret: none

HTTPLogger:
  Enabled: true

VomsmapOverride: |+
  "/osg/Role=NULL/Capability=NULL" osguserl
  "/GLOW/Role=htpc/Capability=NULL" osguserl
  "/hcc/Role=NULL/Capability=NULL" osguserl
  "/cms/*" osguserl
  "/fermilab/*" osguserl
  "/osg/ligo/Role=NULL/Capability=NULL" osguserl
  "/virgo/ligo/Role=NULL/Capability=NULL" osguserl
  "/sdcc/Role=NULL/Capability=NULL" osguserl
  "/sphenix/Role=NULL/Capability=NULL" osguserl
  "/atlas/*" osguserl
  "/Gluex/Role=NULL/Capability=NULL" osguserl
  "/dune/Role=pilot/Capability=NULL" osguserl
  "/icecube/Role=pilot/Capability=NULL" osguserl
  "/xenon.biggrid.nl/Role=NULL/Capability=NULL" osguserl

GridmapOverride: |+
  "/DC=foo/DC=bar/OU=Organic Units/OU=Users/CN=YourUserName" osg

Certificate:
  Secret: null
  
Developer:
  Enabled: false

```

Before deploying the HostedCE, at this point you may want to test your private key and SSH endpoint before deploying the application to SLATE. You may also want to check that job submission is working as expected. For example, here I try to log in with the key (notice that it *does not* require a passphrase, this is a HostedCE requirement!), and I'm able to successfully run a job through my local batch system.

	$ ssh -i osg-keypair osguser@condor.grid.uchicago.edu
	Last login: Mon Oct 14 12:51:23 2019 from cdf38.uchicago.edu
	[osguser@condor ~]$ condor_run /bin/hostname
        htcondor-river-v2-586998c97f-m4vkp
	
This is often the first place to start looking when your HostedCE isn't working, so I would encourage you to test this appropriately at your site.

Once we're happy with the configuration and we have tested basic access, we can ask SLATE to install it:

	slate app install --cluster uchicago-prod --group slate-dev osg-hosted-ce --dev --conf hostedce.yaml 


## Testing the HostedCE
