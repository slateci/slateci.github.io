---
title: "OpenTelemetry in SLATE"
overview: How SLATE uses OpenTelemetry
published: true
permalink: blog/2023-06-30-opentelemetry.html
attribution: Suchandra Thapa
layout: post
type: markdown
tag: draft
---

In order to get better observability of the SLATE components, we
added OpenTelemetry reporting into the SLATE servers.

<!--end_excerpt-->

## Background

OpenTelemetry is a collection of tools and SDKs that allow developers to 
collect information about their applications runtime metrics.  
OpenTelemetry collects`information about duration required to handle user 
requests and about error rates when processing requests.  OpenTelemetry can
correlate and combine this information across multiple services in order to
generate an unified view of user interactions.

We instrumented the SLATE services with OpenTelemetry in order to better 
track user interactions and to help with debugging errors that may occur.

## OpenTelemetry

[OpenTelemetry](https://opentelemetry.io/) in an 
[observability framework](https://opentelemetry.io/docs/concepts/observability-primer/#what-is-observability) 
that encompasses a variety of tools and SDKs that combine to let 
an user's end to end interactions to be recorded even if the interaction spans
multiple services such as web portals, api services, and database calls. 

Generally OpenTelemetry is used by instrumenting applications to collect 
telemetry data (traces, metrics, logs).  This information is then 
sent to a collector that stores the data in a 
time series database.  Finally, there is a frontend that presents information to 
admins.

### Traces and spans

[Traces](https://opentelemetry.io/docs/concepts/signals/traces/) are the primary metric that 
OpenTelemetry is concerned about.  Traces are intended to record all activity that occur in a single
user interaction.  Traces are usually broken down into atomic units of work called spans.   A span might 
consist of something like a SQL query run against a database or a call to a
microservice or an operation on a storage device.  

Traces are usually generated by a `Trace Provider` that is integrated into a service as a SDK.  There
are trace providers for multiple languages such as Python, C++, Javascript, Java, etc.  In SLATE,
we use the C++ and Python trace providers.  

Trace providers will aggregate spans created in an application and then bundle them into traces.  
The traces are then sent to a collector for further processing.

### Collectors

OpenTelemetry uses collectors to receive and process traces from trace providers.  A
collector provides a centralized location for collecting traces from multiple trace providers and
combines related traces so that interactions across different services and sources can be associated 
with a single user interaction.  

Collectors can also apply additional processing to traces that it receives and then stores processed traces 
in persistent storage (usually a time series database like Clickhouse). 


### Signoz

SLATE choose to use [Signoz](https://signoz.io/) to handle the duties of storing and presenting traces. Signoz 
is an open source platform for presenting OpenTelemetry data.  Signoz also provides a chart that deploys 
an OpenTelemetry collector and a Clickhouse database to store traces, metrics, and logs.  Signoz also provides
alerting and monitoring of services based on traces received.

## Instrumenting SLATE

### C++

The SLATE api server is written in C++.  In order to instrument this component, we had to use the 
OpenTelemetry C++ SDK and update the server code to generate and send traces.  Although the process 
was a bit tedious, it was relatively straightforward.

The core of the OpenTelemetry code is located in [Telemetry.cpp](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp).
The `initializeTracer` function is called when the server starts up.  It takes the configuration
settings for the server and then intializes a trace provider for the api server 
with the appropriate collector, sampling parameters, and other settings.

Within each function that is involved in handling incoming api calls, the code then obtains the
trace provider using [getTracer](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L124).
This gets a `shared_ptr` to a tracer object that can generate
spans associated with handling an incoming api call.  If the function is directly handling an incoming
api request (e.g. the web framework routes incoming http requests to this function), it will then
use [setWebSpanAttributes](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L130) 
and [getWebSpanOptions](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L152) to get attributes and options for the span.
These options and attributes are then passed to the `StartSpan` method of the tracer in order to
create a new span that'll cover the work done by this function. [populateSpan](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L179) 
is called right after the 
span is generated to add various information (client ip, http method, etc.) about the incoming http 
request to the span. If an error occurs within the function, 
[setWebSpanError](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L209) is used to populate the span with error information to aid in debugging.  Finally, 
the span's `End()` method is used to close the span and send it to the OpenTelemetry collector.

If the function that's being run isn't directly processing an incoming api call, it does something slightly
different to generate a span. The [getTracer](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L124) 
function is still called to get a `shared_ptr` to
a tracer object.  However, [setInternalSpanAttributes](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L167) 
and [getInternalSpanOptions](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L172) are used
to get the options and attributes for the span.  These are then used when creating a new span using
the `StartSpan` method of the tracer object. If an error occurs during the function call, 
[setSpanError](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L215) is used to set the appropriate fields in the span to aid in debugging.  Finally,
the span's `End()` method is called just before the function exits.

### Python

The SLATE portal uses python and flask to provide a web interface for SLATE.  Unlike with C++, OpenTelemetry
provides a way to autoinstrument python and flask code so that traces are automatically generated.  This is achieved
by deploying an instrumentation CRD with the SLATE portal pods on GKE.  

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: slate-instrumentation
spec:
  exporter:
    endpoint: http://injection-collector-collector.development.svc.cluster.local:4318
  propagators:
    - tracecontext
    - baggage
    - b3
  python:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:latest

```

The CRD sets the endpoint that traces go to  as well as the autoinstrumentation that should be used.  Next,
we update the labels for the pod that runs the portal with annotations that indicate that an OpenTelemetry
sidecar should be deployed

```yaml
       sidecar.opentelemetry.io/inject: "injection-collector"
       instrumentation.opentelemetry.io/inject-python: "true"
```

Finally a CRD that is used to automatically deploy a collector in the same namespace as the portal pods.
This collector is used to collect traces from the portal and then forward it to a central collector.

{% raw %}
```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: injection-collector
spec:
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
    processors:
      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 15
      batch:
        send_batch_size: 10000
        timeout: 10s
    exporters:
      logging:
      otlphttp:
        endpoint: opentel.collector.dns
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: []
          exporters: [logging, otlphttp]
```
{% endraw %}

## Observability

Using Signoz, we can then examine operations on the web portal or within the api server.  We 
can search for interactions based on search criteria like user name, the cluster being worked on,
errors, http codes (e.g. 200, 500, 403, etc.) as well as time taken to handle api calls.  This 
allows us to find anamolous calls that are taking more time than usual or to find api calls that result
in elevated error rates (e.g. due to a problem with a SLATE cluster). 

## Adding monitoring

Signoz also allows us to automatically send alerts to slack channels when incoming traces indicate that
certain api calls result in elevated error rates or require significantly more time than usual to
process a request.  This allows us to proactively investigate potential issues.


##  Conclusion

Although adding OpenTelemetry to the SLATE infrastructure required large changes to our codebase and to 
our infrastructure, the resulting improvements in observability and debugging has improved our ability 
to monitor and respond to problems within the SLATE infrastructure.




